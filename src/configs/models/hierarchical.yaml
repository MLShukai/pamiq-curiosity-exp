# StackedHidden Models

policy_value0:
  _target_: pamiq_core.torch.TorchTrainingModel
  has_inference_model: true
  device: ${shared.device}
  dtype: ${shared.dtype}
  inference_procedure:
    _target_: hydra.utils.get_method
    path: exp.models.policy.StackedHiddenPiVLatent.forward_with_no_len
  model:
    _target_: exp.models.policy.StackedHiddenPiVLatent
    obs_dim: 512
    action_choices:
      _target_: hydra.utils.get_object
      path: exp.envs.vrchat.OSC_ACTION_CHOICES
    dim: ${.core_model.dim}
    core_model:
      _target_: exp.models.components.qlstm.QLSTM
      dim: 256
      dim_ff_hidden: ${python.eval:"${.dim} * 4"}
      depth: 8
      dropout: 0.1

policy_value1:
  _target_: pamiq_core.torch.TorchTrainingModel
  has_inference_model: true
  device: ${shared.device}
  dtype: ${shared.dtype}
  inference_procedure:
    _target_: hydra.utils.get_method
    path: exp.models.policy.StackedHiddenContinuousPiVLatent.forward_with_no_len
  model:
    _target_: exp.models.policy.StackedHiddenContinuousPiVLatent
    obs_dim: 1024
    action_dim: 512
    dim: ${.core_model.dim}
    core_model:
      _target_: exp.models.components.qlstm.QLSTM
      dim: 512
      dim_ff_hidden: ${python.eval:"${.dim} * 4"}
      depth: 8
      dropout: 0.1

policy_value2:
  _target_: pamiq_core.torch.TorchTrainingModel
  has_inference_model: true
  device: ${shared.device}
  dtype: ${shared.dtype}
  inference_procedure:
    _target_: hydra.utils.get_method
    path: exp.models.policy.StackedHiddenContinuousPiVLatent.forward_with_no_len
  model:
    _target_: exp.models.policy.StackedHiddenContinuousPiVLatent
    obs_dim: 1024
    action_dim: 1024
    dim: ${.core_model.dim}
    core_model:
      _target_: exp.models.components.qlstm.QLSTM
      dim: 1024
      dim_ff_hidden: ${python.eval:"${.dim} * 4"}
      depth: 8
      dropout: 0.1

forward_dynamics0:
  _target_: pamiq_core.torch.TorchTrainingModel
  has_inference_model: true
  device: ${shared.device}
  dtype: ${shared.dtype}
  inference_procedure:
    _target_: hydra.utils.get_method
    path: exp.models.forward_dynamics.StackedHiddenLatentFDObsInfo.forward_with_no_len
  model:
    _target_: exp.models.forward_dynamics.StackedHiddenLatentFDObsInfo
    obs_info:
      _target_: exp.models.utils.ObsInfo
      dim: 128 # Same as JEPA instantiattion.
      num_tokens:
        _target_: exp.models.jepa.compute_image_jepa_output_patch_count
        image_size:
          - ${shared.image.height}
          - ${shared.image.width}
        patch_size: 12 # Same as JEPA Instantiation
        output_downsample: 3 # Same as JEPA Instantiation
      dim_hidden: ${..dim}
    action_dim: 256
    dim: ${.core_model.dim}
    core_model:
      _target_: exp.models.components.qlstm.QLSTM
      dim: 256
      dim_ff_hidden: ${python.eval:"${.dim} * 4"}
      depth: 8
      dropout: 0.1

forward_dynamics1:
  _target_: pamiq_core.torch.TorchTrainingModel
  has_inference_model: true
  device: ${shared.device}
  dtype: ${shared.dtype}
  inference_procedure:
    _target_: hydra.utils.get_method
    path: exp.models.forward_dynamics.StackedHiddenLatentFD.forward_with_no_len
  model:
    _target_: exp.models.forward_dynamics.StackedHiddenLatentFD
    obs_dim: 256
    action_dim: 512
    dim: ${.core_model.dim}
    core_model:
      _target_: exp.models.components.qlstm.QLSTM
      dim: 512
      dim_ff_hidden: ${python.eval:"${.dim} * 4"}
      depth: 8
      dropout: 0.1

forward_dynamics2:
  _target_: pamiq_core.torch.TorchTrainingModel
  has_inference_model: true
  device: ${shared.device}
  dtype: ${shared.dtype}
  inference_procedure:
    _target_: hydra.utils.get_method
    path: exp.models.forward_dynamics.StackedHiddenLatentFD.forward_with_no_len
  model:
    _target_: exp.models.forward_dynamics.StackedHiddenLatentFD
    obs_dim: 512
    action_dim: 1024
    dim: ${.core_model.dim}
    core_model:
      _target_: exp.models.components.qlstm.QLSTM
      dim: 1024
      dim_ff_hidden: ${python.eval:"${.dim} * 4"}
      depth: 8
      dropout: 0.1
